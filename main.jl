using GaussianProcesses
using Distributions
using Random
using FFTW
using Distributions
using Plots
using LinearAlgebra
using Turing
using StatsFuns
using FillArrays



#######################################################################
# Idea NR. 1
#######################################################################3

# Inverse fourier transform with fft_seq beeing the transformed sequence and
# n_eval the points we want to evaluate
# We use https://dsp.stackexchange.com/questions/534/how-to-get-coefficients-for-sine-cosine-function-from-complex-fft

function inv_fft(fft_seq, n_eval, q)
    println(q)
    N = length(fft_seq)
    qant = quantile(norm.(fft_seq),q)
    println(sum(norm.(fft_seq) .>= qant))
    res = zeros(length(n_eval))
    for (k,el) in enumerate(fft_seq)
        if norm(el) >=qant
            A = norm(el) * 1/N
            phi = -angle(el)
            res .= res .+ real(A*cos(phi)) .* cos.(π *2* (k-1 )/N .* n_eval ) .+ real(A*sin(phi)) .* sin.(π *2* (k-1 )/N .* n_eval )
        end
    end
    return res
end

# Short verification

vect = Vector{Float64}(bitrand(100))
plot(inv_fft(fft(vect), Vector{Float64}(0:length(vect)-1),0.0))
plot!(vect)

## The model
# We assume that y_i ∼ Bernoulli(θ_i)
# And θ_i = logistic(f_latent / (1+scale))
# With scale ~ Exponential(sqrt(n)) - which is a bit arbitrary

#
# So what are we doing here ? Well, we have a latent process determined by the fourier transform
# Now since we are discarding a lot of frequencies, the latent process does not match the data
# In that case, dividing the latent process by a scale parameter makes the resulting distribtuion flat (i.e. θ -> 0.5)



@model function fft_mdl(y,latent_f)
    # Prior over noise distribution
    # We make this dependent on the size of the data to prevent overfitting
    scale ~ Exponential(sqrt(length(latent_f)))
    #ϵ ~ MvNormal(Fill(μ, length(y)), σ)
    θ = StatsFuns.logistic.(latent_f /  (1 + scale))
    pdist = product_distribution([Bernoulli(θi) for θi in θ])
    y ~ pdist
end



"""
    predict_next_dct(seq, n_pred)

## Arguments
    - seq:      The sequence in the form of a Bool-Vector
    - n_pred:   The amount of elements we want to predict


The return value is a vector with the bernoulli coefficient for every single bit (p(y =1) = θ)

Idea: View the sequence as a time series with a periodic structure, for example 0 1 0 1 0 1 etc.
In this case, we want to automatically detect the structure. We achieve this by making use of the
FFT. Then, the frequencies with the lowest q% of magnitudes are discarded.
Why ? Assume that the data is generated by drawing i.i.d. samples. In such a case, discarding q% of the frequencies will most likeli result in a distinct curve.
On the other hand, in cases where the data does indeed have a periodic strucutre, some frequencies will be dominant and therefore discarding
the low magnitude frequencies should not affect the curve to much.

As a next step, the idea is to define a probabilistic model. Say f_i are the values obtained when expanding the signal obtained when discarding the low magnitude frequencies.
We say that θ_i = logistic(f_i / scale), where the scale parameter is determining how well the latent states f_l represent the data. As a result, by adjusting the scale parameter,
we can control how well we trust the predictions f_i.


"""

function predict_next_fft(seq, n_pred)
    if isempty(seq)
        return 0.5 .* ones(n_pred)
    end
    N = length(seq)
    # We want to have a float vector
    # This scaling is pretty important - intuitively, it determines how sure
    # we are about the sequence.
    fseq = (Vector{Float64}(seq) .- 0.5)*1.0


    # Lets take the FFT

    fft_seq = fft(fseq)

    ## Filtering
    # Discard all lwo frequency terms!
    # we get them using changepoint detection
    #fft_seq[2:Integer(round(length(dct_seq)/4, RoundUp)):end] .= 0.0

    # We want to have at least some frequencies in our model
    q = minimum([ maximum([0.0, 1- 10/N]) , N/100])

    latent_f = inv_fft(fft_seq, Vector{Float64}(0:length(seq)-1), q)


    latent_f_predict = inv_fft(fft_seq,Vector{Float64}(length(seq):length(seq)+n_pred-1),q )
    # Now lets define the model
    chn = sample(fft_mdl(Vector{Bool}(seq),latent_f), PG(20), 5000)
    scale = mean(chn[:scale][1000:end]) # Discard burnin samples
    println(scale)
    #σ = mean(chn[:σ][1000:end]) # Discard burnin samples
    return  logistic.(latent_f_predict./ scale)

end


# We are overfitting, but not that bad...
vect = bitrand(1000)

vect = [0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1]
# Some good working examples
vect = [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1]
vect = [0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1]
vect = vect = [1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
# This does not work, due to an issues with the frequency approach
vect = [0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1]


# Assume we have some noise in the sequence
# The result is ok
vect = [0,0,1,1,0,0,1,1,0,1,1,1,0,1,1,1,0,0,1,1,0,0,1]
vect = [0,1,0,1,0,1,0,1,0,1,0,1,0,1,1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,1,1,0,1,0,1,0,1,0,1,0,1]

plot(predict_next_fft(vect,20))
scatter!(vect)









#######################################################################
# Idea NR. 2
#######################################################################3



"""
    predict_next_gp(seq, n_pred)

## Arguments
    - seq:      The sequence in the form of a Bool-Vector
    - n_pred:   The amount of elements we want to predict

The return value is a vector with the bernoulli coefficient for every single bit (p(y =1) = θ)
Idea: Forgett all before and just create a GP with a kernel with many periods. Then learn it.
This is based on the same intuition as before.

We use https://dsp.stackexchange.com/questions/534/how-to-get-coefficients-for-sine-cosine-function-from-complex-fft

"""

function predict_next_gp(seq, n_pred)

    if isempty(seq)
        return 0.5 .* ones(n_pred)
    end

    N = length(seq)
    # We want to have a float vector
    # This scaling is pretty important - intuitively, it determines how sure
    # we are about the sequence.
    fseq = (Vector{Float64}(seq) .- 0.5)*2.0




    kern = Noise(0.0)

    # How to initalize? Just use the FFT as idea which frequencies are important
    ft = fft(fseq)
    meanf = MeanConst(real(ft[1])/N)

    for k in Integer(round(length(seq)/2, RoundUp)): length(seq)
        kern += Periodic(norm(ft[k]/N),1.0,log(N/ (k-1 )))
    end

    # Just a very peaky kernel



    x = reshape(Vector{Float64}(0:length(seq)-1), (1,:))
    lik = BernLik()   # Bernoulli likelihood for binary data {0,1}
    gp = GP(x,Vector{Bool}(seq), meanf ,kern,lik)      # Fit the Gaussian process model
    try
        optimize!(gp, GradientDescent(), Optim.Options(;iterations=50);kern = true)
    catch e
        nothing
    end
    x_plot = reshape(Vector{Float64}(0:n_pred+length(seq)-1), (1,:))
    predict_y(gp, x_plot)[1]

end






#####
# Some evaluation
####
# Works pretty well actually...
vect = bitrand(1)

# Some good working examples
vect = [0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1]
vect = [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1]
vect = [0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1]
vect = [0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1]

# Also this works fine!
vect = [1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]

vect = [0,0,1,1,0,0,1,1,0,1,1,1,0,1,1,1,0,0,1,1,0,0,1]
# We can see that it is really sensitive to the noise
#  This is really difficult to learn as there is a shift in the Frequency!
vect = [0,1,0,1,0,1,0,1,0,1,0,1,0,1,1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,1,1,0,1,0,1,0,1,0,1,0,1]




plot(predict_next_gp([],20))
scatter!(vect)











#######
## This was a stupid idea
#######

# # This is only to verify the formula
# function dct_test(ft, int_p)
#     N = length(ft)
#     int = 0:length(ft)-1
#     res = zeros(length(int_p))
#     for i in 1:length(int_p)
#         res[i] = sum(ft .* cos.(π / N  .* (int .+ 1/2) .* int_p[i]))
#     end
#     return res
# end
#
#
#
#
# """
#     predict_next_dct(seq, n_pred)
#
# ## Arguments
#     - seq:      The sequence in the form of a Bool-Vector
#     - n_pred:   The amount of elements we want to predict
#
# Idea: View the sequence as a time series with a periodic structure, for example 0 1 0 1 0 1 etc.
# In this case, we want to automatically detect the structure. We make this by taking the
# inverse Discrete Cosine Transform - Type 2. Then, the frequencies with the lowest 25 % magnitudes are discarded.
# This is made based on the following consideration. Assume that the data is generated by an i.i.d. process.
# In such a case, discarding 25% of the frequencies will most likeli result distinct curve. On the other hand,
# in case that the data does indeed have a periodic strucutre, some frequencies will be dominant and therefore discarding
# the low magnitude frequencies should not affect the curve to much.
#
# As a next step, we set the mean function of a GP to be evluation of the DCT-2 transformation of the prviously sparsified frequencies.
# As Kernel, we use a noise kernel + constant kernel and learn the noise and constant. As a result, obtain the predictions from the GP
#
#
# """
#
# function predict_next_dct(seq, n_pred)
#     N = length(seq)
#     # We want to have a float vector
#     # This scaling is pretty important - intuitively, it determines how sure
#     # we are about the sequence.
#     fseq = (Vector{Float64}(seq) .- 0.5)*1.5
#
#
#     # Now lets take the inverse dct transform => So that we can use the
#     # DCT-2 transform to obtain again the signal
#     # Rescale - Necessary to fit the setting of the package FFTW
#     fseqr = fseq[:] ./ sqrt(N)
#     fseqr[2:end] = fseqr[2:end] * 1/sqrt(length(N)) * sqrt(2)
#     dct_seq = idct(fseqr)
#
#     ## Filtering
#
#     meanf = MeanConst(0.0)
#
#     for (k,el) in enumerate(dct_seq)
#         if abs(el) > quantile(abs.(dct_seq),0.5)
#             meanf += MeanPeriodic(el,0.0,log(N*2 / ((k-1) .+ 1/2) ))
#         end
#     end
#
#     # The kernel is simply noise!
#     kern =   Noise(0.0) + Const(0.0)
#
#
#
#     x = reshape(Vector{Float64}(0:length(seq)-1), (1,:))
#     lik = BernLik()   # Bernoulli likelihood for binary data {0,1}
#     gp = GP(x,Vector{Bool}(seq), meanf ,kern,lik)      # Fit the Gaussian process model
#     try
#         optimize!(gp, GradientDescent(), Optim.Options(;iterations=50); noise = true, kern = true, domean =false, noisebounds = [-10.0, 1.0])
#     catch e
#         nothing
#     end
#     x_plot = reshape(Vector{Float64}(length(seq):n_pred+length(seq)-1), (1,:))
#     predict_y(gp, x_plot)[1]
#
# end
#
#
# vect = bitrand(100)
# vect = [0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1]
# vect = [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1]
# vect = [0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1]
# vect = [0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1]
#
# # This one is a clear issue !!
# # Lets try with the FFT
# vect = [1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
#
# plot(predict_next_dct(vect,100))
